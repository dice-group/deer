/*
 * DEER Core Library - DEER - RDF Dataset Enrichment Framework
 * Copyright © 2013 Data Science Group (DICE) (kevin.dressler@uni-paderborn.de)
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Affero General Public License for more details.
 *
 * You should have received a copy of the GNU Affero General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package org.aksw.deer.learning.genetic;

import com.google.common.util.concurrent.AtomicDouble;
import org.aksw.deer.enrichments.LinkingEnrichmentOperator;
import org.aksw.deer.learning.FitnessFunction;
import org.apache.commons.io.FileUtils;
import org.apache.commons.math3.distribution.TDistribution;
import org.apache.commons.math3.exception.MathIllegalArgumentException;
import org.apache.commons.math3.stat.descriptive.SummaryStatistics;
import org.apache.jena.rdf.model.Model;
import org.apache.jena.rdf.model.ModelFactory;
import org.apache.jena.rdf.model.ResourceFactory;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import java.io.FileWriter;
import java.io.IOException;
import java.io.StringReader;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.List;
import java.util.Locale;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.IntStream;

/**
*
*/
public class HPOPhase4 {

  private static TrainingData trainingData;
  private static String testDir = "./test/";
  private static String[] paths = new String[]{
    "a.ttl",
    "b.ttl",
    "c.ttl",
    "d.ttl"
  };

  @BeforeClass
  public static void setUp() throws IOException {
    LinkingEnrichmentOperator.setStaticLearning(()-> new LinkingEnrichmentOperator().createParameterMap()
      .add(LinkingEnrichmentOperator.LINKING_PREDICATE, ResourceFactory.createProperty("<http://my.dataset.edu/birthPlace>"))
      .add(LinkingEnrichmentOperator.LINK_SPECIFICATION, ResourceFactory.createStringLiteral("exactmatch(x.http://dbpedia.org/ontology/birthPlace, y.http://www.w3.org/2000/01/rdf-schema#label)"))
      .add(LinkingEnrichmentOperator.THRESHOLD, ResourceFactory.createTypedLiteral(1.0))
      .init());

    if (!Files.exists(Paths.get(testDir))) {
      Files.createDirectory(Paths.get(testDir));
    }

    Model a = ModelFactory.createDefaultModel().read(new StringReader(
        "<http://dbpedia.org/resource/Peter_Koper> <http://dbpedia.org/ontology/birthDate> \"1947-0-0\"^^<http://www.w3.org/2001/XMLSchema#date> .\n" +
        "<http://dbpedia.org/resource/Peter_Koper> <http://xmlns.com/foaf/0.1/name> \"Peter Koper\"@en .\n" +
        "<http://dbpedia.org/resource/Peter_Koper> <http://dbpedia.org/ontology/birthPlace> \"Quakenbrück\" .\n" +
        "<http://dbpedia.org/resource/Peter_Koper> <http://www.w3.org/2000/01/rdf-schema#comment> \"Peter Koper (born 1947) is an American journalist, professor, screenwriter, and producer. He numbers among the original Dreamlanders, the group of actors and artists who worked with independent film maker John Waters on his early films. He has written for the Associated Press, the Baltimore Sun, American Film, Rolling Stone, and People. He worked as a staff writer and producer for America's Most Wanted, and has written television for the Discovery Channel, the Learning Channel, Paramount Television and Lorimar Television. Koper wrote and co-produced the cult movie Headless Body in Topless Bar, and wrote the screenplay for Island of the Dead. He has taught at the University of the District of Columbia, and Hofstra University.\" .\n" +
        "<http://dbpedia.org/resource/Peter_Koper> <http://dbpedia.org/ontology/producerOf> <http://dbpedia.org/resource/Island_of_the_Dead_(2000_film)> .\n" +
        "<http://dbpedia.org/resource/Peter_Koper> <http://dbpedia.org/ontology/almaMater> <http://dbpedia.org/resource/Johns_Hopkins_University> .\n" +
        ""), null, "NT");

    Model b = ModelFactory.createDefaultModel().read(new StringReader(
      "<http://example.org/Quakenbrück> <http://dbpedia.org/ontology/postalCode> \"49610\" .\n" +
        "<http://example.org/Quakenbrück> <http://dbpedia.org/ontology/country> <http://dbpedia.org/resource/Germany> .\n" +
        "<http://example.org/Quakenbrück> <http://www.w3.org/2003/01/geo/wgs84_pos#long> \"7.9574999809265136719\"^^<http://www.w3.org/2001/XMLSchema#float> .\n" +
        "<http://example.org/Quakenbrück> <http://www.w3.org/2003/01/geo/wgs84_pos#lat> \"52.67722320556640625\"^^<http://www.w3.org/2001/XMLSchema#float> .\n" +
        "<http://example.org/Quakenbrück> <http://www.w3.org/2000/01/rdf-schema#label> \"Quakenbrück\" .\n" +
        "<http://example.org/Quakenbrück> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://dbpedia.org/ontology/Town> ." +
        ""), null, "NT");
  // (a -> )(b -> AC ->) -> MERGE -> NER -> PC -> DEREF
    Model target = ModelFactory.createDefaultModel().read(new StringReader(
      "<http://dbpedia.org/resource/Island_of_the_Dead_(2000_film)> <http://dbpedia.org/ontology/director> <http://dbpedia.org/resource/Tim_Southam> .\n" +
        "<http://my.dataset.edu/Peter_Koper> <http://my.dataset.edu/relatedTo> <http://dbpedia.org/resource/Paramount_Pictures> .\n" +
        "<http://my.dataset.edu/Peter_Koper> <http://my.dataset.edu/birthPlace> <http://example.org/Quakenbrück> .\n" +
        "<http://my.dataset.edu/Peter_Koper> <http://my.dataset.edu/relatedTo> <http://dbpedia.org/resource/Baltimore> .\n" +
        "<http://my.dataset.edu/Peter_Koper> <http://my.dataset.edu/name> \"Peter Koper\"@en .\n" +
        "<http://my.dataset.edu/Peter_Koper> <http://dbpedia.org/ontology/producerOf> <http://dbpedia.org/resource/Island_of_the_Dead_(2000_film)> .\n" +
        "<http://example.org/Quakenbrück> <http://www.w3.org/2003/01/geo/wgs84_pos#lat> \"52.67722320556640625\"^^<http://www.w3.org/2001/XMLSchema#float> .\n" +
        "<http://example.org/Quakenbrück> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://dbpedia.org/ontology/Town> .\n" +
        "<http://example.org/Quakenbrück> <http://www.w3.org/2000/01/rdf-schema#label> \"Quakenbrück\" .\n" +
        "<http://example.org/Quakenbrück> <http://www.w3.org/2003/01/geo/wgs84_pos#long> \"7.9574999809265136719\"^^<http://www.w3.org/2001/XMLSchema#float> .\n" +
        "<http://example.org/Quakenbrück> <http://dbpedia.org/ontology/country> <http://dbpedia.org/resource/Germany> .\n"
    ), null, "NT");
    a.write(new FileWriter(testDir + paths[0]), "TTL");
    b.write(new FileWriter(testDir + paths[1]), "TTL");
    target.write(new FileWriter(testDir + paths[2]), "TTL");
    trainingData = new TrainingData(new FitnessFunction(new int[]{1,1,1,1}, 1), List.of(testDir + paths[0], testDir + paths[1]), List.of(testDir + paths[0], testDir + paths[1]), testDir + paths[2], testDir + paths[2], testDir + paths[3]);
    Genotype.SIZE = 10;
  }

  private static double calcMeanCI(SummaryStatistics stats, double level) {
    try {
      // Create T Distribution with N-1 degrees of freedom
      TDistribution tDist = new TDistribution(stats.getN() - 1);
      // Calculate critical value
      double critVal = tDist.inverseCumulativeProbability(1.0 - (1 - level) / 2);
      // Calculate confidence interval
      return critVal * stats.getStandardDeviation() / Math.sqrt(stats.getN());
    } catch (MathIllegalArgumentException e) {
      return Double.NaN;
    }
  }

  @AfterClass
  public static void tearDown() throws IOException {
    if (Files.exists(Paths.get(testDir))) {
      FileUtils.deleteDirectory(Paths.get(testDir).toFile());
    }
  }

  @Test
  public void simpleTest() {
    for (int i = 0; i<100; i++) {
      List<PopulationEvaluationResult> evaluationResults = getAlg(1.0, 0.5, 0.5).run();
      System.out.println(evaluationResults.size());
  //      Phenotype of = Phenotype.of(evaluationResults.get(evaluationResults.size() - 1).getBest());
  //      Genotype genotype = evaluationResults.get(evaluationResults.size() - 1).getBest().compactBestResult(false, 0);
  //      System.out.println(of);
  //      System.out.println(genotype);
  //      System.out.println(evaluationResults.get(evaluationResults.size() - 1).getBest().getBestFitness());
    }
  }

  @Test
  public void constructorTest() {
    runPerformanceExperiment(1.0,0.5,0.25);
  }

  private void runPerformanceExperiment(double oF, double mP, double mR) {
    int it = 50;
    double[][] results = new double[it][0];
    double maxGenGlobal = 0;
    for (int j = 0; j < it; j++) {
      results[j] = getAlg(oF, mP, mR).run().stream()
        .mapToDouble(PopulationEvaluationResult::getMax).toArray();
      if (results[j].length > maxGenGlobal) {
        maxGenGlobal = results[j].length;
      }
    }

    for (int g = 0; g < maxGenGlobal; g++) {
      SummaryStatistics stats = new SummaryStatistics();
      for (int j = 0; j < it; j++) {
        if (g < results[j].length ) {
          stats.addValue(results[j][g]);
        } else {
          stats.addValue(results[j][results[j].length-1]);
        }
      }
      // Calculate 95% confidence interval
      double ci = calcMeanCI(stats, 0.95);
      double lower = stats.getMax();
      double upper = stats.getMin();
      System.out.println(String.format("%d %.6f %.6f %.6f %.6f", g, stats.getMean(), ci, lower, upper));
    }




  //    System.out.println(String.format(Locale.ENGLISH, "%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%d", oF, mP, mR, statistics.getAverage(), statistics.getStandardDeviation(), statistics.getMin(), statistics.getMax(), d.get()/it, i.get()));
  }

  private void runSimpleExperiment(double oF, double mP, double mR) {
    int it = 1;
    AtomicInteger i = new AtomicInteger(0);
    AtomicDouble d = new AtomicDouble(0);
    PopulationEvaluationResult.DoubleStatistics statistics = IntStream.range(0, it)
      .mapToDouble(j -> {
        GeneticProgrammingAlgorithm alg = getAlg(oF, mP, mR);
        List<PopulationEvaluationResult> evaluationResults = alg.run();
        double max = evaluationResults.get(evaluationResults.size() - 1).getMax();
        d.addAndGet(max);
        if (max == 1.0) {
          i.incrementAndGet();
        }
        return evaluationResults.size();
      })
      .collect(PopulationEvaluationResult.DoubleStatistics::new,
        PopulationEvaluationResult.DoubleStatistics::accept,
        PopulationEvaluationResult.DoubleStatistics::combine);
    System.out.println(String.format(Locale.ENGLISH, "%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f\t%d", oF, mP, mR, statistics.getAverage(), statistics.getStandardDeviation(), statistics.getMin(), statistics.getMax(), d.get()/it, i.get()));
  //    System.out.println(oF  + "\t" +  mP  + "\t" +  mR + "\t" + statistics.getAverage() + "\t" + statistics.getStandardDeviation() + "\t" + statistics.getMax());
  }

  private GeneticProgrammingAlgorithm getAlg(double oF, double mP, double mR) {
    return new GeneticProgrammingAlgorithm(
      new Population(30, () -> new RandomGenotype(trainingData)),
      trainingData.getFitnessFunction(),
      new TournamentSelector(3, 0.75),
      List.of(new SemanticRecombinator()),
      oF,
      List.of(new SimpleSemanticMutator(), new AllMutator()),
      mP,
      mR
    );
  }

}